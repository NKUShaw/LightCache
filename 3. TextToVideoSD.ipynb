{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "639d5866-2381-4917-861d-8c6e1302d99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from diffusers import TextToVideoSDPipeline\n",
    "import numpy as np\n",
    "from torchvision.transforms import Resize, ToTensor, Compose\n",
    "from diffusers.utils import export_to_gif, load_image, export_to_video\n",
    "from huggingface_hub import hf_hub_download\n",
    "from safetensors.torch import load_file\n",
    "from LightCache.LightCache import LightCacher\n",
    "from fme import FMEWrapper\n",
    "from time import time\n",
    "from PIL import Image, ImageSequence\n",
    "import lpips\n",
    "from typing import List\n",
    "import cv2\n",
    "from skimage.metrics import structural_similarity as ssim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70cc3b67-75ca-4616-83f2-206b551e1b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_frames(frames, size=(224, 224)):\n",
    "    transform = Compose([\n",
    "        Resize(size),\n",
    "        ToTensor(),                     # (C, H, W), range [0,1]\n",
    "        lambda x: x * 2 - 1             # normalize to [-1, 1]\n",
    "    ])\n",
    "    return [transform(f) for f in frames]\n",
    "\n",
    "def compute_lpips(frames1, frames2):\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    lpips_model = lpips.LPIPS(net='alex').to(device)\n",
    "    scores = []\n",
    "    frames1_tensor = preprocess_frames(frames1, size=(224, 224))\n",
    "    frames2_tensor = preprocess_frames(frames2, size=(224, 224))\n",
    "    for i in range(len(frames1_tensor)):\n",
    "        f1 = frames1_tensor[i].unsqueeze(0).to(device)\n",
    "        f2 = frames2_tensor[i].unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            score = lpips_model(f1, f2).item()\n",
    "        scores.append(score)\n",
    "    return np.mean(scores)\n",
    "\n",
    "def gif_to_frames(gif_path):\n",
    "    with Image.open(gif_path) as im:\n",
    "        frames = [frame.convert(\"RGB\").copy() for frame in ImageSequence.Iterator(im)]\n",
    "    return frames\n",
    "\n",
    "def video_to_frames(video_path: str) -> List[Image.Image]:\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "\n",
    "    if not cap.isOpened():\n",
    "        raise ValueError(f\"Failed to open video file: {video_path}\")\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        # OpenCV: BGR â†’ RGB\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        pil_image = Image.fromarray(frame_rgb)\n",
    "        frames.append(pil_image)\n",
    "\n",
    "    cap.release()\n",
    "    return frames\n",
    "\n",
    "def calculate_psnr(img1, img2):\n",
    "    mse = np.mean((img1 - img2) ** 2)\n",
    "    if mse == 0:\n",
    "        return float('inf')  \n",
    "    PIXEL_MAX = 255.0\n",
    "    return 20 * np.log10(PIXEL_MAX / np.sqrt(mse))\n",
    "\n",
    "def compute_ssim(frames1, frames2):\n",
    "    scores = []\n",
    "    for img1, img2 in zip(frames1, frames2):\n",
    "        img1_np = np.array(img1.resize((256, 256))).astype(np.float32)\n",
    "        img2_np = np.array(img2.resize((256, 256))).astype(np.float32)\n",
    "\n",
    "        if img1_np.ndim == 3:\n",
    "            # For RGB images, compute mean SSIM over channels\n",
    "            ssim_val = 0\n",
    "            for c in range(3):\n",
    "                ssim_val += ssim(img1_np[:, :, c], img2_np[:, :, c], data_range=255)\n",
    "            ssim_val /= 3\n",
    "        else:\n",
    "            ssim_val = ssim(img1_np, img2_np, data_range=255)\n",
    "\n",
    "        scores.append(ssim_val)\n",
    "    return np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7ffcffb-a44e-461f-971a-eab5cb44c9d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0898847bba3a4b05a6488a3538d901db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "device = \"cuda\"\n",
    "dtype = torch.float16\n",
    "pipe = TextToVideoSDPipeline.from_pretrained(\"damo-vilab/text-to-video-ms-1.7b\", torch_dtype=torch.float16).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01d1a221-3ced-4ab2-b7bd-0253be6487cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "generator = torch.Generator(device=device).manual_seed(seed)\n",
    "torch.cuda.reset_peak_memory_stats(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0406b94-9028-4c2d-a7a1-70d40e2bcf1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cacher = LightCacher(pipe)\n",
    "cacher.set_params(cache_interval=2, cache_branch_id=0)\n",
    "cacher.enable(Swap=False, Slice=False, Chunk=True)\n",
    "\n",
    "# cacher = FMEWrapper(num_temporal_chunk=9, num_spatial_chunk=2, num_frames=25)\n",
    "# cacher.wrap(pipe)\n",
    "torch.cuda.reset_peak_memory_stats(device)\n",
    "\n",
    "start_time = time()\n",
    "video_frames = pipe(\"A girl smiling\", num_inference_steps=25, num_frames=25,\n",
    "                   generator=generator).frames\n",
    "print(time() - start_time)\n",
    "# export_to_video(video_frames[0], \"./generated_videos/T2V_baseline.mp4\", fps=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b65af046-ecf2-4f49-b036-7d64e60f5029",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Peak memory allocated: 8162.40 MB\n",
      "Peak memory reserved: 11908.00 MB\n"
     ]
    }
   ],
   "source": [
    "peak_mem_alloc = torch.cuda.max_memory_allocated(device) / 1024 ** 2  # MB\n",
    "peak_mem_reserved = torch.cuda.max_memory_reserved(device) / 1024 ** 2  # MB\n",
    "\n",
    "print(f\"Peak memory allocated: {peak_mem_alloc:.2f} MB\")\n",
    "print(f\"Peak memory reserved: {peak_mem_reserved:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1368759-f357-46a2-b493-358ac9ea30ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffusion",
   "language": "python",
   "name": "diffusion"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
