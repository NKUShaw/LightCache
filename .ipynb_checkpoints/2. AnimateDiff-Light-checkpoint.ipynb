{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "639d5866-2381-4917-861d-8c6e1302d99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import lpips\n",
    "from diffusers.models.autoencoders.autoencoder_kl import AutoencoderKL, AutoencoderKLOutput\n",
    "from diffusers.models.autoencoders.vae import Decoder, DecoderOutput, DiagonalGaussianDistribution, Encoder\n",
    "from typing import Union, Tuple\n",
    "from diffusers import AnimateDiffPipeline, MotionAdapter, EulerDiscreteScheduler\n",
    "from torchvision.transforms import Resize, ToTensor, Compose\n",
    "from diffusers.utils import export_to_gif, load_image\n",
    "from huggingface_hub import hf_hub_download\n",
    "from safetensors.torch import load_file\n",
    "from LightCache.LightCache import LightCacher\n",
    "from fme import FMEWrapper\n",
    "from time import time\n",
    "from PIL import Image, ImageSequence\n",
    "import numpy as np\n",
    "from skimage.metrics import structural_similarity as ssim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70cc3b67-75ca-4616-83f2-206b551e1b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_frames(frames, size=(256, 256)):\n",
    "    transform = Compose([\n",
    "        Resize(size),\n",
    "        ToTensor(),                     # (C, H, W), range [0,1]\n",
    "        lambda x: x * 2 - 1             # normalize to [-1, 1]\n",
    "    ])\n",
    "    return [transform(f) for f in frames]\n",
    "\n",
    "def compute_lpips(frames1, frames2):\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    lpips_model = lpips.LPIPS(net='alex').to(device)\n",
    "    scores = []\n",
    "    frames1_tensor = preprocess_frames(frames1, size=(224, 224))\n",
    "    frames2_tensor = preprocess_frames(frames2, size=(224, 224))\n",
    "    for i in range(len(frames1_tensor)):\n",
    "        f1 = frames1_tensor[i].unsqueeze(0).to(device)\n",
    "        f2 = frames2_tensor[i].unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            score = lpips_model(f1, f2).item()\n",
    "        scores.append(score)\n",
    "    print(scores)\n",
    "    return np.mean(scores)\n",
    "\n",
    "def gif_to_frames(gif_path):\n",
    "    with Image.open(gif_path) as im:\n",
    "        frames = [frame.convert(\"RGB\").copy() for frame in ImageSequence.Iterator(im)]\n",
    "    return frames\n",
    "\n",
    "def calculate_psnr(img1, img2):\n",
    "    mse = np.mean((img1 - img2) ** 2)\n",
    "    if mse == 0:\n",
    "        return float('inf')  \n",
    "    PIXEL_MAX = 255.0\n",
    "    return 20 * np.log10(PIXEL_MAX / np.sqrt(mse))\n",
    "\n",
    "def compute_ssim(frames1, frames2):\n",
    "    scores = []\n",
    "    for img1, img2 in zip(frames1, frames2):\n",
    "        img1_np = np.array(img1.resize((256, 256))).astype(np.float32)\n",
    "        img2_np = np.array(img2.resize((256, 256))).astype(np.float32)\n",
    "\n",
    "        if img1_np.ndim == 3:\n",
    "            # For RGB images, compute mean SSIM over channels\n",
    "            ssim_val = 0\n",
    "            for c in range(3):\n",
    "                ssim_val += ssim(img1_np[:, :, c], img2_np[:, :, c], data_range=255)\n",
    "            ssim_val /= 3\n",
    "        else:\n",
    "            ssim_val = ssim(img1_np, img2_np, data_range=255)\n",
    "\n",
    "        scores.append(ssim_val)\n",
    "    print(scores)\n",
    "    return np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92799b76-96e6-43be-a962-2615b95685de",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\"\n",
    "dtype = torch.float16\n",
    "\n",
    "step = 8  # Options: [1,2,4,8]\n",
    "repo = \"ByteDance/AnimateDiff-Lightning\"\n",
    "ckpt = f\"animatediff_lightning_{step}step_diffusers.safetensors\"\n",
    "base = \"emilianJR/epiCRealism\"  # Choose to your favorite base model.\n",
    "\n",
    "adapter = MotionAdapter().to(device, dtype)\n",
    "adapter.load_state_dict(load_file(hf_hub_download(repo ,ckpt), device=device))\n",
    "pipe = AnimateDiffPipeline.from_pretrained(base, motion_adapter=adapter, torch_dtype=dtype).to(device)\n",
    "pipe.scheduler = EulerDiscreteScheduler.from_config(pipe.scheduler.config, timestep_spacing=\"trailing\", beta_schedule=\"linear\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c5a624e-0347-4183-bcd2-77075c494c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "cacher = LightCacher(pipe, 25)\n",
    "cacher.set_params(cache_interval=2, cache_branch_id=0)\n",
    "cacher.enable(Swap=False, Slice=False, Chunk=False)\n",
    "\n",
    "# cacher = FMEWrapper(num_temporal_chunk=9, num_spatial_chunk=2, num_frames=25)\n",
    "# cacher.wrap(pipe)\n",
    "torch.cuda.reset_peak_memory_stats(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a464d15-4bf3-47a0-80fe-67e85d9c4390",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "generator = torch.Generator(device=device).manual_seed(seed)\n",
    "\n",
    "start_time = time()\n",
    "output = pipe(prompt=\"A girl smiling\", guidance_scale=1.0, \n",
    "              num_inference_steps=step, num_frames=25, generator=generator)\n",
    "\n",
    "print(time() - start_time)\n",
    "# export_to_gif(output.frames[0], \"AnimateDiff_Baseline.gif\")\n",
    "# peak_mem_alloc = torch.cuda.max_memory_allocated(device) / 1024 ** 2  # MB\n",
    "peak_mem_reserved = torch.cuda.max_memory_reserved(device) / 1024 ** 2  # MB\n",
    "\n",
    "# print(f\"Peak memory allocated: {peak_mem_alloc:.2f} MB\")\n",
    "print(f\"Peak memory reserved: {peak_mem_reserved:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93a7f22-1f2b-4d76-a060-7880b57e4c90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07bfe934-e6c4-4373-8ea8-103adb513cf3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffusion",
   "language": "python",
   "name": "diffusion"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
